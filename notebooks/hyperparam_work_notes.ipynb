{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "* Three examples of Hyperparameter optimization with deep learning:\n",
    "  * Commercial building power consumption forcasting\n",
    "  * Image classification\n",
    "    * CFAR10\n",
    "    * CFAR100\n",
    "    * MNIST Handwritten digits\n",
    "  * Weather forcasting\n",
    "    * Maybe kaggle?\n",
    "    * pug\n",
    "  \n",
    "## Secondary goal\n",
    "* Get hyperopt integrated with scikit-learn\n",
    "\n",
    "## Strategy\n",
    "* Choose well known data sets (see scikit or skdata)\n",
    "* Get basic model up and running (baseline)\n",
    "* Add hyperopt or other param search\n",
    "* Compare results\n",
    "* Add in NN (maybe for feature selection?)\n",
    "\n",
    "## Next step\n",
    "* Create benchmark\n",
    "  * On NN lib\n",
    "    * Sklearn\n",
    "    * keras\n",
    "  * On param search techniques\n",
    "    * sklearn\n",
    "      * grid search\n",
    "      * random search\n",
    "  * on data sets\n",
    "    * sklearn\n",
    "    * skdata\n",
    "    * keras\n",
    "    \n",
    "## Relevant links\n",
    "* [optimizing params with hyperopt](http://fastml.com/optimizing-hyperparams-with-hyperopt/)\n",
    "* [automatic hyperparam tuning methods](http://www.johnmyleswhite.com/notebook/2012/07/21/automatic-hyperparameter-tuning-methods/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit classifier\n",
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Keras has the MNIST data set as well as an example CNN script [here](https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py) (keras also has an mlp and irnn script).\n",
    "\n",
    "Running the cnn script takes *way* too long on my computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit\n",
    "In light of the keras NNs taking too long to run, as well as the added complexity of training NNs, i've decided to first work with some simple classifiers and stack the hyperparameter optimizers on top before I add another layer of complexity that comes with NNs. Furthermore, this might help us separate the performance gains coming from just the optimizers as opposed to introducing NNs.\n",
    "\n",
    "Scikit has some form of the MNIST dataset as well [here](https://scikit-learn.github.io/dev/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits).\n",
    "\n",
    "Furthermore, scikit has a number of examples about using hyperparam optimizers on digit recognizing classifiers (even an example about using a restricted boltzmann machine for feature selection). I should get some of these up and running to establish a baseline.\n",
    "\n",
    "Idea: Could I get hyperopt working on some of these examples as well? If I can run examples using grid and random search, then I could use that as a hyperparam tuning baseline and compare using hyperopt and other libs as well.\n",
    "\n",
    "### Relevant Links\n",
    "* [Recognizing handwritten digits with svm](https://scikit-learn.github.io/dev/auto_examples/classification/plot_digits_classification.html)\n",
    "* [Parameter estimation using grid search with cross-validation and svc](https://scikit-learn.github.io/dev/auto_examples/model_selection/grid_search_digits.html#example-model-selection-grid-search-digits-py)\n",
    "* [Restricted Boltzmann Machine features for digit classification using logistic regression](https://scikit-learn.github.io/dev/auto_examples/neural_networks/plot_rbm_logistic_classification.html#example-neural-networks-plot-rbm-logistic-classification-py)  # NOTE: The params were found previously using gridsearch.\n",
    "* [Comparing randomized search and grid search for hyperparameter estimation using random forest](https://scikit-learn.github.io/dev/auto_examples/model_selection/randomized_search.html#example-model-selection-randomized-search-py)\n",
    "* [Overview on Grid Search: Searching for estimator parameters](https://scikit-learn.github.io/dev/modules/grid_search.html#grid-search)\n",
    "* [Overview on Randomized Search](https://scikit-learn.github.io/dev/modules/generated/sklearn.grid_search.RandomizedSearchCV.html#sklearn.grid_search.RandomizedSearchCV)\n",
    "\n",
    "### Logistic Regression with Grid Search and Random Search\n",
    "\n",
    "I ran this on the original sklearn data set but since the set was so small, the hyperparameter optimizers didn't do much to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Logistic Regression =============\n",
      "Logistic regression took 0.76 seconds.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        45\n",
      "          1       0.88      0.88      0.88        52\n",
      "          2       0.96      0.96      0.96        53\n",
      "          3       0.93      0.96      0.95        54\n",
      "          4       0.98      1.00      0.99        48\n",
      "          5       0.98      0.96      0.97        57\n",
      "          6       0.95      0.98      0.97        60\n",
      "          7       1.00      0.94      0.97        53\n",
      "          8       0.90      0.90      0.90        61\n",
      "          9       0.95      0.93      0.94        57\n",
      "\n",
      "avg / total       0.95      0.95      0.95       540\n",
      "\n",
      "============= Grid Search =============\n",
      "GridSearchCV took 64.44 seconds for 15 candidate parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.955 (std: 0.004)\n",
      "Parameters: {'C': 0.5, 'intercept_scaling': 0.10000000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.955 (std: 0.004)\n",
      "Parameters: {'C': 0.5, 'intercept_scaling': 0.55000000000000004, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.955 (std: 0.004)\n",
      "Parameters: {'C': 0.5, 'intercept_scaling': 0.77500000000000002, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 0.5, 'intercept_scaling': 0.10000000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.955 (+/-0.009) for {'C': 0.5, 'intercept_scaling': 0.10000000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.955 (+/-0.008) for {'C': 0.5, 'intercept_scaling': 0.32500000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.955 (+/-0.009) for {'C': 0.5, 'intercept_scaling': 0.55000000000000004, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.955 (+/-0.009) for {'C': 0.5, 'intercept_scaling': 0.77500000000000002, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.955 (+/-0.008) for {'C': 0.5, 'intercept_scaling': 1.0, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.951 (+/-0.014) for {'C': 1, 'intercept_scaling': 0.10000000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.951 (+/-0.014) for {'C': 1, 'intercept_scaling': 0.32500000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.951 (+/-0.014) for {'C': 1, 'intercept_scaling': 0.55000000000000004, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.951 (+/-0.013) for {'C': 1, 'intercept_scaling': 0.77500000000000002, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.953 (+/-0.013) for {'C': 1, 'intercept_scaling': 1.0, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.950 (+/-0.019) for {'C': 1.5, 'intercept_scaling': 0.10000000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.950 (+/-0.019) for {'C': 1.5, 'intercept_scaling': 0.32500000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.949 (+/-0.019) for {'C': 1.5, 'intercept_scaling': 0.55000000000000004, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.951 (+/-0.022) for {'C': 1.5, 'intercept_scaling': 0.77500000000000002, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "0.954 (+/-0.016) for {'C': 1.5, 'intercept_scaling': 1.0, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.0001, 'class_weight': None}\n",
      "()\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        45\n",
      "          1       0.89      0.92      0.91        52\n",
      "          2       0.96      0.98      0.97        53\n",
      "          3       0.95      0.96      0.95        54\n",
      "          4       0.98      0.98      0.98        48\n",
      "          5       0.98      0.96      0.97        57\n",
      "          6       0.97      0.98      0.98        60\n",
      "          7       0.98      0.94      0.96        53\n",
      "          8       0.92      0.89      0.90        61\n",
      "          9       0.93      0.93      0.93        57\n",
      "\n",
      "avg / total       0.95      0.95      0.95       540\n",
      "\n",
      "\n",
      "============= Random Search =============\n",
      "RandomizedSearchCV took 44.37 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.955 (std: 0.007)\n",
      "Parameters: {'C': 1, 'intercept_scaling': 1.0, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001, 'class_weight': None}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.955 (std: 0.006)\n",
      "Parameters: {'C': 1, 'intercept_scaling': 0.32500000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001, 'class_weight': None}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.955 (std: 0.006)\n",
      "Parameters: {'C': 1, 'intercept_scaling': 0.10000000000000001, 'max_iter': 100, 'penalty': 'l1', 'tol': 0.001, 'class_weight': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation as cval\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from time import time\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "        \n",
    "# Initialize the data        \n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "X_train, X_test, y_train, y_test = cval.train_test_split(digits.data, digits.target, test_size=0.3, random_state=0)\n",
    "\n",
    "# Logistic regression without hyperparameter optimization\n",
    "\n",
    "logit = LogisticRegression()\n",
    "\n",
    "start = time()\n",
    "logit.fit(X_train, y_train)\n",
    "#logit.score(X_test, y_test)\n",
    "\n",
    "print(\"============= Logistic Regression =============\")\n",
    "print(\"Logistic regression took %.2f seconds.\"\n",
    "      % (time() - start))\n",
    "print\n",
    "print(classification_report(y_test, logit.predict(X_test)))\n",
    "\n",
    "# Define parameter space to search over\n",
    "# Note: GridSearch uses ParameterGrid method which does NOT interpolate between values.\n",
    "# See grid_search.ParameterSampler for a random distribution sampler.\n",
    "# Note: Use logit.get_params() to see default params.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "param_dist = {\"C\":[0.1,2],\n",
    "              \"intercept_scaling\":[.02,2],\n",
    "              \"max_iter\":[50],\n",
    "              \"tol\":[0.00001,0.001]\n",
    "             }\n",
    "\"\"\"\n",
    "\n",
    "C_param = list(np.linspace(0.1,2,5))\n",
    "iter_num = [100]\n",
    "inter_scale = list(np.linspace(0.1,1,5))\n",
    "\n",
    "param_dist = {\"C\":[.5,1,1.5],\n",
    "              \"intercept_scaling\":inter_scale,\n",
    "              \"max_iter\":iter_num,\n",
    "              \"tol\":[.0001],\n",
    "              \"penalty\":['l1'],\n",
    "              \"class_weight\":[None]\n",
    "             }\n",
    "# Initialize grid search\n",
    "\n",
    "#grid_search = GridSearchCV(logit, param_dist)\n",
    "grid_search = GridSearchCV(logit, param_grid=param_dist)\n",
    "\n",
    "# Run grid search and check results\n",
    "start = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"============= Grid Search =============\")\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.grid_scores_)))\n",
    "report(grid_search.grid_scores_)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print\n",
    "print(grid_search.best_params_)\n",
    "print\n",
    "print(\"Grid scores on development set:\")\n",
    "print\n",
    "for params, mean_score, scores in grid_search.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean_score, scores.std() * 2, params))\n",
    "print()\n",
    "\n",
    "print(\"Detailed classification report:\")\n",
    "print\n",
    "print(\"The model is trained on the full development set.\")\n",
    "print(\"The scores are computed on the full evaluation set.\")\n",
    "print\n",
    "y_true, y_pred = y_test, grid_search.predict(X_test)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print\n",
    "\n",
    "# Random Search\n",
    "\n",
    "rand_C_param = sp_randint(1,3)\n",
    "\n",
    "rand_param_dist = {\"C\":sp_randint(1,3),           # Want range(0.5,1.5)\n",
    "              \"intercept_scaling\":inter_scale,\n",
    "              \"max_iter\":iter_num,\n",
    "              \"tol\":[0.001],\n",
    "              \"penalty\":['l1'],\n",
    "              \"class_weight\":[None]\n",
    "             }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(logit, param_distributions=rand_param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"============= Random Search =============\")\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Benchmarking methods\n",
    "\n",
    "* Question: I'm confused about how to benchmark hyperparam opts against non-hyperparam opt problems since (my **assumption** is that) in the non-hyperparam case, our choice of initial hyperparameters is arbitrary and therefore arbitrarily good or bad. In the case of the above logit regression with or without hyperparam opts, we found very little change by adding the hyperparam opts in suggesting (another **assumption**:) that the logit regression was already close to global optimum. \n",
    "  * Remark: If our solution space is convex, I would imagine that we wouldn't get much of an improvement from a hyperparam opt.\n",
    "  * Working solution: Try simple convex and non-convex functions. Can I make the initial guess of parameters bad?\n",
    "  \n",
    "### How does Bergstra benchmark hyperopt?\n",
    "* Bergstra benchmarked his hyperopt using HPOLib (which is part of the AutoML lib). \n",
    "  * [Bergstra's paper](http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf)\n",
    "  * [HPOLib](http://www.automl.org/hpolib)\n",
    "  * [List of benchmarks used by HPOLib](http://www.automl.org/benchmarks.html)\n",
    "    * Note that the first benchmarks are simple functions: \"Branin, Camelback and the Hartmann 6d function are three simple test functions, which are easy and cheap to evaluate.\" \n",
    "      * More [simple functions](http://www.sfu.ca/~ssurjano/optimization.html)\n",
    "    * Also note that there is a logistic regression benchmark classifying MNIST (cool coincidence!)\n",
    "      * The logit regression uses hyperopt-NNs. Not sure if current version of hyperopt though.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
