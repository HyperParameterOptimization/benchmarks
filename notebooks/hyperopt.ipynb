{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import skdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'c2': 0.06428850816311964}\n",
      "('case 2', 0.06428850816311964)\n"
     ]
    }
   ],
   "source": [
    "# define an objective function\n",
    "def objective(args):\n",
    "    case, val = args\n",
    "    if case == 'case 1':\n",
    "        return val\n",
    "    else:\n",
    "        return val ** 2\n",
    "\n",
    "# define a search space\n",
    "from hyperopt import hp\n",
    "space = hp.choice('a',\n",
    "    [\n",
    "        ('case 1', 1 + hp.lognormal('c1', 0, 1)),\n",
    "        ('case 2', hp.uniform('c2', -10, 10))\n",
    "    ])\n",
    "\n",
    "# minimize the objective over the space\n",
    "from hyperopt import fmin, tpe\n",
    "best = fmin(objective, space, algo=tpe.suggest, max_evals=100)\n",
    "\n",
    "print best\n",
    "# -> {'a': 1, 'c2': 0.01420615366247227}\n",
    "print hyperopt.space_eval(space, best)\n",
    "# -> ('case 2', 0.01420615366247227}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page is a tutorial on basic usage of `hyperopt.fmin()`.\n",
    "It covers how to write an objective function that fmin can optimize, and how to describe a search space that fmin can search.\n",
    "\n",
    "Hyperopt's job is to find the best value of a scalar-valued, possibly-stochastic function over a set of possible arguments to that function.\n",
    "Whereas many optimization packages will assume that these inputs are drawn from a vector space,\n",
    "Hyperopt is different in that it encourages you to describe your search space in more detail.\n",
    "By providing more information about where your function is defined, and where you think the best values are, you allow algorithms in hyperopt to search more efficiently.\n",
    "\n",
    "The way to use hyperopt is to describe:\n",
    "\n",
    "* the objective function to minimize\n",
    "* the space over which to search\n",
    "* the database in which to store all the point evaluations of the search\n",
    "* the search algorithm to use\n",
    "\n",
    "This (most basic) tutorial will walk through how to write functions and search spaces,\n",
    "using the default `Trials` database, and the dummy `random` search algorithm.\n",
    "Section (1) is about the different calling conventions for communication between an objective function and hyperopt.\n",
    "Section (2) is about describing search spaces.\n",
    "\n",
    "Parallel search is possible when replacing the `Trials` database with\n",
    "a `MongoTrials` one;\n",
    "there is another wiki page on the subject of [using mongodb for parallel search](Parallelizing-Evaluations-During-Search-via-MongoDB).\n",
    "\n",
    "Choosing the search algorithm is as simple as passing `algo=hyperopt.tpe.suggest` instead of `algo=hyperopt.random.suggest`.\n",
    "The search algorithms are actually callable objects, whose constructors\n",
    "accept configuration arguments, but that's about all there is to say about the\n",
    "mechanics of choosing a search algorithm.\n",
    "\n",
    "## 1. Defining a Function to Minimize\n",
    "\n",
    "Hyperopt provides a few levels of increasing flexibility / complexity when it comes to specifying an objective function to minimize.\n",
    "The questions to think about as a designer are\n",
    "* Do you want to save additional information beyond the function return value, such as other statistics and diagnostic information collected during the computation of the objective?\n",
    "* Do you want to use optimization algorithms that require more than the function value?\n",
    "* Do you want to communicate between parallel processes? (e.g. other workers, or the minimization algorithm)\n",
    "\n",
    "The next few sections will look at various ways of implementing an objective\n",
    "function that minimizes a quadratic objective function over a single variable.\n",
    "In each section, we will be searching over a bounded range from -10 to +10,\n",
    "which we can describe with a *search space*:\n",
    "```\n",
    "space = hp.uniform('x', -10, 10)\n",
    "```\n",
    "\n",
    "Below, Section 2, covers how to specify search spaces that are more complicated.\n",
    "\n",
    "### 1.1 The Simplest Case\n",
    "\n",
    "The simplest protocol for communication between hyperopt's optimization\n",
    "algorithms and your objective function, is that your objective function\n",
    "receives a valid point from the search space, and returns the floating-point\n",
    "*loss* (aka negative utility) associated with that point.\n",
    "\n",
    "\n",
    "```python\n",
    "from hyperopt import fmin, tpe, hp\n",
    "best = fmin(fn=lambda x: x ** 2,\n",
    "    space=hp.uniform('x', -10, 10),\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100)\n",
    "print best\n",
    "```\n",
    "\n",
    "This protocol has the advantage of being extremely readable and quick to\n",
    "type. As you can see, it's nearly a one-liner.\n",
    "The disadvantages of this protocol are\n",
    "(1) that this kind of function cannot return extra information about each evaluation into the trials database,\n",
    "and\n",
    "(2) that this kind of function cannot interact with the search algorithm or other concurrent function evaluations.\n",
    "You will see in the next examples why you might want to do these things.\n",
    "\n",
    "\n",
    "### 1.2 Attaching Extra Information via the Trials Object\n",
    "\n",
    "If your objective function is complicated and takes a long time to run, you will almost certainly want to save more statistics\n",
    "and diagnostic information than just the one floating-point loss that comes out at the end.\n",
    "For such cases, the fmin function is written to handle dictionary return values.\n",
    "The idea is that your loss function can return a nested dictionary with all the statistics and diagnostics you want.\n",
    "The reality is a little less flexible than that though: when using mongodb for example,\n",
    "the dictionary must be a valid JSON document.\n",
    "Still, there is lots of flexibility to store domain specific auxiliary results.\n",
    "\n",
    "When the objective function returns a dictionary, the fmin function looks for some special key-value pairs\n",
    "in the return value, which it passes along to the optimization algorithm.\n",
    "There are two mandatory key-value pairs:\n",
    "* `status` - one of the keys from `hyperopt.STATUS_STRINGS`, such as 'ok' for\n",
    "  successful completion, and 'fail' in cases where the function turned out to\n",
    "  be undefined.\n",
    "* `loss` - the float-valued function value that you are trying to minimize, if\n",
    "  the status is 'ok' then this has to be present.\n",
    "\n",
    "The fmin function responds to some optional keys too:\n",
    "\n",
    "* `attachments` -  a dictionary of key-value pairs whose keys are short\n",
    "  strings (like filenames) and whose values are potentially long strings (like\n",
    "  file contents) that should not be loaded from a database every time we\n",
    "  access the record. (Also, MongoDB limits the length of normal key-value\n",
    "  pairs so once your value is in the megabytes, you may *have* to make it an\n",
    "  attachment.)\n",
    "* `loss_variance` - float - the uncertainty in a stochastic objective function\n",
    "* `true_loss` - float -\n",
    "  When doing hyper-parameter optimization, if you store the generalization error of your model with this name, then you can sometimes get spiffier output from the built-in plotting routines.\n",
    "* `true_loss_variance` - float - the uncertainty in the generalization error\n",
    "\n",
    "Since dictionary is meant to go with a variety of back-end storage\n",
    "mechanisms, you should make sure that it is JSON-compatible.  As long as it's\n",
    "a tree-structured graph of dictionaries, lists, tuples, numbers, strings, and\n",
    "date-times, you'll be fine.\n",
    "\n",
    "**HINT:** To store numpy arrays, serialize them to a string, and consider storing\n",
    "them as attachments.\n",
    "\n",
    "Writing the function above in dictionary-returning style, it\n",
    "would look like this:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK\n",
    "\n",
    "def objective(x):\n",
    "    return {'loss': x ** 2, 'status': STATUS_OK }\n",
    "\n",
    "best = fmin(objective,\n",
    "    space=hp.uniform('x', -10, 10),\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100)\n",
    "\n",
    "print best\n",
    "```\n",
    "\n",
    "### 1.3 The Trials Object\n",
    "\n",
    "To really see the purpose of returning a dictionary,\n",
    "let's modify the objective function to return some more things,\n",
    "and pass an explicit `trials` argument to `fmin`.\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def objective(x):\n",
    "    return {\n",
    "        'loss': x ** 2,\n",
    "        'status': STATUS_OK,\n",
    "        # -- store other results like this\n",
    "        'eval_time': time.time(),\n",
    "        'other_stuff': {'type': None, 'value': [0, 1, 2]},\n",
    "        # -- attachments are handled differently\n",
    "        'attachments':\n",
    "            {'time_module': pickle.dumps(time.time)}\n",
    "        }\n",
    "trials = Trials()\n",
    "best = fmin(objective,\n",
    "    space=hp.uniform('x', -10, 10),\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials)\n",
    "\n",
    "print best\n",
    "```\n",
    "\n",
    "In this case the call to fmin proceeds as before, but by passing in a trials object directly,\n",
    "we can inspect all of the return values that were calculated during the experiment.\n",
    "\n",
    "So for example:\n",
    "* `trials.trials` - a list of dictionaries representing everything about the search\n",
    "* `trials.results` - a list of dictionaries returned by 'objective' during the search\n",
    "* `trials.losses()` - a list of losses (float for each 'ok' trial)\n",
    "* `trials.statuses()` - a list of status strings\n",
    "\n",
    "This trials object can be saved, passed on to the built-in plotting routines,\n",
    "or analyzed with your own custom code.\n",
    "\n",
    "The *attachments* are handled by a special mechanism that makes it possible to use the same code\n",
    "for both `Trials` and `MongoTrials`.\n",
    "\n",
    "You can retrieve a trial attachment like this, which retrieves the 'time_module' attachment of the 5th trial:\n",
    "```python\n",
    "msg = trials.trial_attachments(trials.trials[5])['time_module']\n",
    "time_module = pickle.loads(msg)\n",
    "```\n",
    "\n",
    "The syntax is somewhat involved because the idea is that attachments are large strings,\n",
    "so when using MongoTrials, we do not want to download more than necessary.\n",
    "Strings can also be attached globally to the entire trials object via trials.attachments,\n",
    "which behaves like a string-to-string dictionary.\n",
    "\n",
    "\n",
    "**N.B.** Currently, the trial-specific attachments to a Trials object are tossed into the same global trials attachment dictionary, but that may change in the future and it is not true of MongoTrials.\n",
    "\n",
    "\n",
    "\n",
    "### 1.4 The Ctrl Object for Realtime Communication with MongoDB\n",
    "\n",
    "It is possible for `fmin()` to give your objective function a handle to the mongodb used by a parallel experiment. This mechanism makes it possible to update the database with partial results, and to communicate with other concurrent processes that are evaluating different points.\n",
    "Your objective function can even add new search points, just like `random.suggest`.\n",
    "\n",
    "The basic technique involves:\n",
    "\n",
    "* Using the `fmin_pass_expr_memo_ctrl` decorator\n",
    "* call `pyll.rec_eval` in your own function to build the search space point\n",
    "  from `expr` and `memo`.\n",
    "* use `ctrl`, an instance of `hyperopt.Ctrl` to communicate with the live\n",
    "  trials object.\n",
    "\n",
    "It's normal if this doesn't make a lot of sense to you after this short tutorial,\n",
    "but I wanted to give some mention of what's possible with the current code base,\n",
    "and provide some terms to grep for in the hyperopt source, the unit test,\n",
    "and example projects, such as [hyperopt-convnet](https://github.com/jaberg/hyperopt-convnet).\n",
    "Email me or file a github issue if you'd like some help getting up to speed with this part of the code.\n",
    "\n",
    "\n",
    "## 2. Defining a Search Space\n",
    "\n",
    "A search space consists of nested function expressions, including stochastic expressions.\n",
    "The stochastic expressions are the hyperparameters.\n",
    "Sampling from this nested stochastic program defines the random search algorithm.\n",
    "The hyperparameter optimization algorithms work by replacing normal \"sampling\" logic with\n",
    "adaptive exploration strategies, which make no attempt to actually sample from the distributions specified in the search space.\n",
    "\n",
    "It's best to think of search spaces as stochastic argument-sampling programs. For example\n",
    "```python\n",
    "from hyperopt import hp\n",
    "space = hp.choice('a',\n",
    "    [\n",
    "        ('case 1', 1 + hp.lognormal('c1', 0, 1)),\n",
    "        ('case 2', hp.uniform('c2', -10, 10))\n",
    "    ])\n",
    "```\n",
    "The result of running this code fragment is a variable `space` that refers to a graph of expression identifiers and their arguments.\n",
    "Nothing has actually been sampled, it's just a graph describing *how* to sample a point.\n",
    "The code for dealing with this sort of expression graph is in `hyperopt.pyll` and I will refer to these graphs as *pyll graphs* or *pyll programs*.\n",
    "\n",
    "If you like, you can evaluate a sample space by sampling from it.\n",
    "```python\n",
    "import hyperopt.pyll.stochastic\n",
    "print hyperopt.pyll.stochastic.sample(space)\n",
    "```\n",
    "\n",
    "This search space described by `space` has 3 parameters:\n",
    "* 'a' - selects the case\n",
    "* 'c1' - a positive-valued parameter that is used in 'case 1'\n",
    "* 'c2' - a bounded real-valued parameter that is used in 'case 2'\n",
    "\n",
    "One thing to notice here is that every optimizable stochastic expression has a *label* as the first argument.\n",
    "These labels are used to return parameter choices to the caller, and in various ways internally as well.\n",
    "\n",
    "A second thing to notice is that we used tuples in the middle of the graph (around each of 'case 1' and 'case 2').\n",
    "Lists, dictionaries, and tuples are all upgraded to \"deterministic function expressions\" so that they can be part of the search space stochastic program.\n",
    "\n",
    "A third thing to notice is the numeric expression `1 + hp.lognormal('c1', 0, 1)`, that is embedded into the description of the search space.\n",
    "As far as the optimization algorithms are concerned, there is no difference between adding the 1 directly in the search space\n",
    "and adding the 1 within the logic of the objective function itself.\n",
    "As the designer, you can choose where to put this sort of processing to achieve the kind modularity you want.\n",
    "Note that the intermediate expression results within the search space can be arbitrary Python objects, even when optimizing in parallel using mongodb.\n",
    "It is easy to add new types of non-stochastic expressions to a search space description, see below (Section 2.3) for how to do it.\n",
    "\n",
    "A fourth thing to note is that 'c1' and 'c2' are examples what we will call *conditional parameters*.\n",
    "Each of 'c1' and 'c2' only figures in the returned sample for a particular value of 'a'.\n",
    "If 'a' is 0, then 'c1' is used but not 'c2'.\n",
    "If 'a' is 1, then 'c2' is used but not 'c1'.\n",
    "Whenever it makes sense to do so, you should encode parameters as conditional ones this way,\n",
    "rather than simply ignoring parameters in the objective function.\n",
    "If you expose the fact that 'c1' sometimes has no effect on the objective function (because it has no effect on the argument to the objective function) then search can be more efficient about credit assignment.\n",
    "\n",
    "\n",
    "### 2.1 Parameter Expressions\n",
    "\n",
    "The stochastic expressions currently recognized by hyperopt's optimization algorithms are:\n",
    "\n",
    "* `hp.choice(label, options)`\n",
    "   * Returns one of the options, which should be a list or tuple.\n",
    "       The elements of `options` can themselves be [nested] stochastic expressions.\n",
    "       In this case, the stochastic choices that only appear in some of the options become *conditional* parameters.\n",
    "\n",
    "* `hp.randint(label, upper)`\n",
    "   * Returns a random integer in the range [0, upper). The semantics of this\n",
    "       distribution is that there is *no* more correlation in the loss function between nearby integer values,\n",
    "       as compared with more distant integer values.  This is an appropriate distribution for describing random seeds    for example.\n",
    "       If the loss function is probably more correlated for nearby integer values, then you should probably use one of the \"quantized\" continuous distributions, such as either `quniform`, `qloguniform`, `qnormal` or `qlognormal`.\n",
    "\n",
    "* `hp.uniform(label, low, high)`\n",
    "   * Returns a value uniformly between `low` and `high`.\n",
    "   * When optimizing, this variable is constrained to a two-sided interval.\n",
    "\n",
    "* `hp.quniform(label, low, high, q)`\n",
    "    * Returns a value like round(uniform(low, high) / q) * q\n",
    "    * Suitable for a discrete value with respect to which the objective is still somewhat \"smooth\", but which should be bounded both above and below.\n",
    "\n",
    "* `hp.loguniform(label, low, high)`\n",
    "    * Returns a value drawn according to exp(uniform(low, high)) so that the logarithm of the return value is uniformly distributed.\n",
    "    * When optimizing, this variable is constrained to the interval [exp(low), exp(high)].\n",
    "\n",
    "* `hp.qloguniform(label, low, high, q)`\n",
    "    * Returns a value like round(exp(uniform(low, high)) / q) * q\n",
    "    * Suitable for a discrete variable with respect to which the objective is \"smooth\" and gets smoother with the size of the value, but which should be bounded both above and below.\n",
    "\n",
    "* `hp.normal(label, mu, sigma)`\n",
    "    * Returns a real value that's normally-distributed with mean mu and standard deviation sigma. When optimizing, this is an unconstrained variable.\n",
    "\n",
    "* `hp.qnormal(label, mu, sigma, q)`\n",
    "    * Returns a value like round(normal(mu, sigma) / q) * q\n",
    "    * Suitable for a discrete variable that probably takes a value around mu, but is fundamentally unbounded.\n",
    "\n",
    "* `hp.lognormal(label, mu, sigma)`\n",
    "    * Returns a value drawn according to exp(normal(mu, sigma)) so that the logarithm of the return value is normally distributed.\n",
    "        When optimizing, this variable is constrained to be positive.\n",
    "\n",
    "* `hp.qlognormal(label, mu, sigma, q)`\n",
    "    * Returns a value like round(exp(normal(mu, sigma)) / q) * q\n",
    "    * Suitable for a discrete variable with respect to which the objective is smooth and gets smoother with the size of the variable, which is bounded from one side.\n",
    "\n",
    "### 2.2 A Search Space Example: scikit-learn\n",
    "\n",
    "To see all these possibilities in action, let's look at how one might go about describing the space of hyperparameters of classification algorithms in scikit-learn.\n",
    "(This idea is being developed in [hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn))\n",
    "\n",
    "```python\n",
    "from hyperopt import hp\n",
    "space = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'naive_bayes',\n",
    "    },\n",
    "    {\n",
    "        'type': 'svm',\n",
    "        'C': hp.lognormal('svm_C', 0, 1),\n",
    "        'kernel': hp.choice('svm_kernel', [\n",
    "            {'ktype': 'linear'},\n",
    "            {'ktype': 'RBF', 'width': hp.lognormal('svm_rbf_width', 0, 1)},\n",
    "            ]),\n",
    "    },\n",
    "    {\n",
    "        'type': 'dtree',\n",
    "        'criterion': hp.choice('dtree_criterion', ['gini', 'entropy']),\n",
    "        'max_depth': hp.choice('dtree_max_depth',\n",
    "            [None, hp.qlognormal('dtree_max_depth_int', 3, 1, 1)]),\n",
    "        'min_samples_split': hp.qlognormal('dtree_min_samples_split', 2, 1, 1),\n",
    "    },\n",
    "    ])\n",
    "```\n",
    "\n",
    "\n",
    "### 2.3 Adding Non-Stochastic Expressions with pyll\n",
    "\n",
    "You can use such nodes as arguments to pyll functions (see pyll).\n",
    "File a github issue if you want to know more about this.\n",
    "\n",
    "In a nutshell, you just have to decorate a top-level (i.e. pickle-friendly) function so\n",
    "that it can be used via the `scope` object.\n",
    "\n",
    "```python\n",
    "import hyperopt.pyll\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "@scope.define\n",
    "def foo(a, b=0):\n",
    "     print 'runing foo', a, b\n",
    "     return a + b / 2\n",
    "\n",
    "# -- this will print 0, foo is called as usual.\n",
    "print foo(0)\n",
    "\n",
    "# In describing search spaces you can use `foo` as you\n",
    "# would in normal Python. These two calls will not actually call foo,\n",
    "# they just record that foo should be called to evaluate the graph.\n",
    "\n",
    "space1 = scope.foo(hp.uniform('a', 0, 10))\n",
    "space2 = scope.foo(hp.uniform('a', 0, 10), hp.normal('b', 0, 1)\n",
    "\n",
    "# -- this will print an pyll.Apply node\n",
    "print space1\n",
    "\n",
    "# -- this will draw a sample by running foo()\n",
    "print hyperopt.pyll.stochastic.sample(space1)\n",
    "```\n",
    "\n",
    "\n",
    "### 2.4 Adding New Kinds of Hyperparameter\n",
    "\n",
    "Adding new kinds of stochastic expressions for describing parameter search spaces should be avoided if possible.\n",
    "In order for all search algorithms to work on all spaces, the search algorithms must agree on the kinds of hyperparameter that describe the space.\n",
    "As the maintainer of the library, I am open to the possibility that some kinds of expressions should be added from time to time, but like I said, I would like to avoid it as much as possible.\n",
    "Adding new kinds of stochastic expressions is not one of the ways hyperopt is meant to be extensible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
